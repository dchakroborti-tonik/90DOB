{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "223b46d5",
   "metadata": {},
   "source": [
    "# <div align = \"center\" style=\"color:rgb(10, 250, 150);\"> Dima Data </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10790cb1",
   "metadata": {},
   "source": [
    "10aug_snapshot\tALL the users who did not have any disbursed loans until 10th Aug 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04575c5",
   "metadata": {},
   "source": [
    "# Define Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c07f1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# # Jupyter Notebook Loading Header\n",
    "#\n",
    "# This is a custom loading header for Jupyter Notebooks in Visual Studio Code.\n",
    "# It includes common imports and settings to get you started quickly.\n",
    "# %% [markdown]\n",
    "## Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import joblib\n",
    "import uuid\n",
    "\n",
    "import gcsfs\n",
    "import duckdb as dd\n",
    "\n",
    "\n",
    "\n",
    "path = r'C:\\Users\\Dwaipayan\\AppData\\Roaming\\gcloud\\legacy_credentials\\dchakroborti@tonikbank.com\\adc.json'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path\n",
    "client = bigquery.Client(project='prj-prod-dataplatform')\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"prj-prod-dataplatform\"\n",
    "# %% [markdown]\n",
    "## Configure Settings\n",
    "# Set options or configurations as needed\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"Display.max_rows\", 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7ce215",
   "metadata": {},
   "source": [
    "# Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54b83624",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DATE = datetime.now().strftime(\"%Y%m%d\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fd4e8d",
   "metadata": {},
   "source": [
    "# <div align=\"left\" style=\"color:rgb(51, 250, 250);\"> Functions </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52b3cd0",
   "metadata": {},
   "source": [
    "## <div align=\"left\" style=\"color:rgb(51, 250, 250);\"> Save the data to google clound storage </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b5c4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_to_gcs(df, bucket_name, destination_blob_name, file_format='csv'):\n",
    "    \"\"\"Saves a pandas DataFrame to Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        df: The pandas DataFrame to save.\n",
    "        bucket_name: The name of the GCS bucket.\n",
    "        destination_blob_name: The name of the blob to be created.\n",
    "        file_format: The file format to save the DataFrame in ('csv' or 'parquet').\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a temporary file\n",
    "    if file_format == 'csv':\n",
    "        temp_file = 'temp.csv'\n",
    "        df.to_csv(temp_file, index=False)\n",
    "    elif file_format == 'parquet':\n",
    "        temp_file = 'temp.parquet'\n",
    "        df.to_parquet(temp_file, index=False)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid file format. Please choose 'csv' or 'parquet'.\")\n",
    "\n",
    "    # Upload the file to GCS\n",
    "    storage_client = storage.Client(project=\"prj-prod-dataplatform\")\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(temp_file)\n",
    "\n",
    "    # Remove the temporary file\n",
    "    import os\n",
    "    os.remove(temp_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98075ddb",
   "metadata": {},
   "source": [
    "## <div align=\"left\" style=\"color:rgb(51, 250, 250);\"> Read the Data from Google Cloud Storage </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a96cadd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df_from_gcs(bucket_name, source_blob_name, file_format='csv'):\n",
    "    \"\"\"Reads a DataFrame from Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        bucket_name: The name of the GCS bucket.\n",
    "        source_blob_name: The name of the blob to read.\n",
    "        file_format: The file format to read ('csv' or 'parquet').\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The data loaded from the GCS file.\n",
    "    \"\"\"\n",
    "    # Create a temporary file name\n",
    "    temp_file = f'temp.{file_format}'\n",
    "    \n",
    "    try:\n",
    "        # Initialize GCS client\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "\n",
    "        # Download the file to a temporary location\n",
    "        blob.download_to_filename(temp_file)\n",
    "\n",
    "        # Read the file into a DataFrame\n",
    "        if file_format == 'csv':\n",
    "            df = pd.read_csv(temp_file, low_memory=False)\n",
    "        elif file_format == 'parquet':\n",
    "            df = pd.read_parquet(temp_file)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid file format. Please choose 'csv' or 'parquet'.\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    finally:\n",
    "        # Clean up the temporary file\n",
    "        if os.path.exists(temp_file):\n",
    "            os.remove(temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787030fd",
   "metadata": {},
   "source": [
    "## <div align = \"left\" style=\"color:rgb(51, 250, 250);\"> Data Quality Report </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6e8d405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality_report(df, target_col='ln_fspd30_flag'):\n",
    "    # Initialize an empty list to store each row of data\n",
    "    report_data = []\n",
    "    # Iterate over each column in the DataFrame to compute metrics\n",
    "    for col in df.columns:\n",
    "        # Determine the data type of the column\n",
    "        data_type = df[col].dtype\n",
    "       \n",
    "        # Calculate the number of missing values in the column\n",
    "        missing_values = df[col].isnull().sum()\n",
    "       \n",
    "        # Calculate the percentage of missing values relative to the total number of rows\n",
    "        missing_percentage = (missing_values / len(df)) * 100\n",
    "       \n",
    "        # Calculate the number of unique values in the column\n",
    "        unique_values = df[col].nunique()\n",
    "       \n",
    "        # Calculate the percentage of non-missing values\n",
    "        non_missing_percentage = ((len(df) - missing_values) / len(df)) * 100\n",
    "       \n",
    "        # Check if the column is numeric to compute additional metrics\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            # Compute minimum, maximum, mean, median, mode, mode percentage, standard deviation, and quantiles\n",
    "            min_value = df[col].min()\n",
    "            max_value = df[col].max()\n",
    "            mean_value = df[col].mean()\n",
    "            median_value = df[col].median()\n",
    "            mode_value = df[col].mode().iloc[0] if not df[col].mode().empty else None\n",
    "            mode_percentage = (df[col] == mode_value).sum() / len(df) * 100 if mode_value is not None else None\n",
    "            std_dev = df[col].std()\n",
    "            quantile_25 = df[col].quantile(0.25)\n",
    "            quantile_50 = df[col].quantile(0.50)  # Same as median\n",
    "            quantile_75 = df[col].quantile(0.75)\n",
    "            \n",
    "            # Calculate the Interquartile Range (IQR)\n",
    "            iqr = quantile_75 - quantile_25\n",
    "            \n",
    "            # Calculate Skewness and Kurtosis\n",
    "            skewness = df[col].skew()\n",
    "            kurtosis = df[col].kurt()\n",
    "            \n",
    "            # Calculate Coefficient of Variation (CV) - standardized measure of dispersion\n",
    "            cv = (std_dev / mean_value) * 100 if mean_value != 0 else None\n",
    "            \n",
    "            # Calculate correlation with target variable if target exists in dataframe\n",
    "            if target_col in df.columns and col != target_col and pd.api.types.is_numeric_dtype(df[target_col]):\n",
    "                # Calculate correlation only using rows where both columns have non-null values\n",
    "                correlation = df[[col, target_col]].dropna().corr().iloc[0, 1]\n",
    "            else:\n",
    "                correlation = None\n",
    "        else:\n",
    "            # Assign None for non-numeric columns where appropriate\n",
    "            min_value = None\n",
    "            max_value = None\n",
    "            mean_value = None\n",
    "            median_value = None\n",
    "            mode_value = df[col].mode().iloc[0] if not df[col].mode().empty else None\n",
    "            mode_percentage = (df[col] == mode_value).sum() / len(df) * 100 if mode_value is not None else None\n",
    "            std_dev = None\n",
    "            quantile_25 = None\n",
    "            quantile_50 = None\n",
    "            quantile_75 = None\n",
    "            iqr = None\n",
    "            skewness = None\n",
    "            kurtosis = None\n",
    "            cv = None\n",
    "            correlation = None\n",
    "       \n",
    "        # Append the computed metrics for the current column to the list\n",
    "        report_data.append({\n",
    "            'Column': col,\n",
    "            'Data Type': data_type,\n",
    "            'Missing Values': missing_values,\n",
    "            'Missing Percentage': missing_percentage,\n",
    "            'Unique Values': unique_values,\n",
    "            'Min': min_value,\n",
    "            'Max': max_value,\n",
    "            'Mean': mean_value,\n",
    "            'Median': median_value,\n",
    "            'Mode': mode_value,\n",
    "            'Mode Percentage': mode_percentage,\n",
    "            'Std Dev': std_dev,\n",
    "            'Non-missing Percentage': non_missing_percentage,\n",
    "            '25% Quantile': quantile_25,\n",
    "            '50% Quantile': quantile_50,\n",
    "            '75% Quantile': quantile_75,\n",
    "            'IQR': iqr,\n",
    "            'Skewness': skewness,\n",
    "            'Kurtosis': kurtosis,\n",
    "            'CV (%)': cv,\n",
    "            f'Correlation with {target_col}': correlation\n",
    "        })\n",
    "    # Create the DataFrame from the list of dictionaries\n",
    "    report = pd.DataFrame(report_data)\n",
    "   \n",
    "    # Return the complete data quality report DataFrame\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c253d6c3",
   "metadata": {},
   "source": [
    "# <div align = \"left\" style=\"color:rgb(51,250,250);\"> Upload pickle file to Google Cloud Storage Bucke </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84f3d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_gcs(bucket_name, source_file_path, destination_blob_name):\n",
    "    \"\"\"Uploads a file to Google Cloud Storage\"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    \n",
    "    blob.upload_from_filename(source_file_path)\n",
    "    print(f\"File {source_file_path} uploaded to {bucket_name}/{destination_blob_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bfe1f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import io\n",
    "from google.cloud import storage\n",
    "def save_pickle_to_gcs(data, bucket_name, destination_blob_name):\n",
    "    \"\"\"\n",
    "    Save any Python object as a pickle file to Google Cloud Storage\n",
    "    \n",
    "    Args:\n",
    "        data: The Python object to pickle (DataFrame, dict, list, etc.)\n",
    "        bucket_name: Name of the GCS bucket\n",
    "        destination_blob_name: Path/filename in the bucket\n",
    "    \"\"\"\n",
    "    # Initialize the GCS client\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    \n",
    "    # Serialize the data to pickle format in memory\n",
    "    pickle_buffer = io.BytesIO()\n",
    "    pickle.dump(data, pickle_buffer)\n",
    "    pickle_buffer.seek(0)\n",
    "    \n",
    "    # Upload the pickle data to GCS\n",
    "    blob.upload_from_file(pickle_buffer, content_type='application/octet-stream')\n",
    "    print(f\"Pickle file uploaded to gs://{bucket_name}/{destination_blob_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aae5788",
   "metadata": {},
   "source": [
    "## worktable_data_analysis.Latest_credo_before_90DOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07b09e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table  worktable_data_analysis.Latest_credo_before_90DOB  created successfully\n"
     ]
    }
   ],
   "source": [
    "sq = f\"\"\" \n",
    "create or replace table worktable_data_analysis.Latest_credo_before_90DOB as\n",
    "with a1 as \n",
    "(select distinct customerId customerid, credo_ref_no, credo_inquiry_date , credo_inquiry_date_manila, onboarding_date, ostype, sourcetab sources\n",
    "from\n",
    " (select distinct trench2.customer_id customerid,'NULL' digitalLoanAccountId,refno credo_ref_no,createdOn credo_inquiry_date, datetime(timestamp(createdOn), 'Asia/Manila') as credo_inquiry_date_manila ,date(ln_90DOB_date) as ln_90DOB_date, sourcetab, onboarding_date \n",
    " ,OFDATECLOSED, coalesce(credolab_ostype.ostype,'android') ostype\n",
    " from\n",
    "(select userId,refno,deviceId,createdOn,B.cust_id from  `dl_loans_db_raw.tdbk_credolab_track` A  join  `dl_customers_db_raw.tdbk_customer_mtb` B on A.userId=B.user_id )  tsa\n",
    "join prj-prod-dataplatform.core_raw.customer_accounts ca on ca.OFCUSTOMERID = tsa.cust_id and ca.CRINTERCODE = 'POC20000'  and (ca.OFDATECLOSED = '1970-01-01' or ca.OFDATECLOSED is null)\n",
    "join  ((select * from worktable_data_analysis.All_90DOB_snapshot)) trench2\n",
    "on tsa.cust_id=trench2.customer_id\n",
    "left join ((select distinct deviceId,'android' as ostype from credolab_raw.android_credolab_datasets_struct_columns\n",
    "union all\n",
    "select distinct deviceId,'ios' as ostype from  credolab_raw.ios_credolab_datasets_struct_columns)) credolab_ostype\n",
    " on credolab_ostype.deviceId=refno\n",
    " where  date(createdOn)<date(ln_90DOB_date)\n",
    " qualify row_number()over (partition by cust_id order by date(createdOn) desc )=1)A\n",
    ")\n",
    "select * from a1 where credo_ref_no is not null\n",
    " ;\n",
    " \"\"\"\n",
    "\n",
    "job = client.query(sq)\n",
    "job.result()  # Wait for the job to complete.\n",
    "time.sleep(5) # Delays for 30 seconds\n",
    "print(f'Table  worktable_data_analysis.Latest_credo_before_90DOB  created successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41db1ab",
   "metadata": {},
   "source": [
    "## worktable_data_analysis.Latest_credo_before_180DOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a81e88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table  worktable_data_analysis.Latest_credo_before_180DOB  created successfully\n"
     ]
    }
   ],
   "source": [
    "sq = f\"\"\" \n",
    "create or replace table worktable_data_analysis.Latest_credo_before_180DOB as\n",
    "with a1 as \n",
    "(select \n",
    "distinct customerId customerid, credo_ref_no, credo_inquiry_date , credo_inquiry_date_manila, onboarding_date, ostype, sourcetab sources\n",
    "from\n",
    " (select distinct trench2.customer_id customerid,'NULL' digitalLoanAccountId,refno credo_ref_no,createdOn credo_inquiry_date, datetime(timestamp(createdOn), 'Asia/Manila') as credo_inquiry_date_manila ,date(ln_180DOB_date) as ln_180DOB_date, sourcetab, onboarding_date \n",
    " ,OFDATECLOSED,  coalesce(credolab_ostype.ostype,'android') ostype\n",
    " from\n",
    "(select userId,refno,deviceId,createdOn,B.cust_id from  `dl_loans_db_raw.tdbk_credolab_track` A  join  `dl_customers_db_raw.tdbk_customer_mtb` B on A.userId=B.user_id )  tsa\n",
    "join prj-prod-dataplatform.core_raw.customer_accounts ca on ca.OFCUSTOMERID = tsa.cust_id and ca.CRINTERCODE = 'POC20000'\n",
    " and (ca.OFDATECLOSED = '1970-01-01' or ca.OFDATECLOSED is null)\n",
    "join  ((select * from worktable_data_analysis.All_180DOB_snapshot)) trench2\n",
    "on tsa.cust_id=trench2.customer_id\n",
    "left join ((select distinct deviceId,'android' as ostype from credolab_raw.android_credolab_datasets_struct_columns\n",
    "union all\n",
    "select distinct deviceId,'ios' as ostype from  credolab_raw.ios_credolab_datasets_struct_columns)) credolab_ostype\n",
    " on credolab_ostype.deviceId=refno\n",
    " where  date(createdOn)<date(ln_180DOB_date)\n",
    "  qualify row_number()over (partition by cust_id order by date(createdOn) desc )=1)A\n",
    ")\n",
    "select * from a1 where credo_ref_no is not null\n",
    " ;\n",
    " \"\"\"\n",
    "\n",
    "job = client.query(sq)\n",
    "job.result()  # Wait for the job to complete.\n",
    "time.sleep(5) # Delays for 30 seconds\n",
    "print(f'Table  worktable_data_analysis.Latest_credo_before_180DOB  created successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fe5b4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table  worktable_data_analysis.Latest_credo_before_10augustsnapshot  created successfully\n"
     ]
    }
   ],
   "source": [
    "sq = f\"\"\" \n",
    "create or replace table worktable_data_analysis.Latest_credo_before_10augustsnapshot as\n",
    "with a1 as \n",
    "(select customerId customerid, credo_ref_no, credo_inquiry_date , credo_inquiry_date_manila, onboarding_date, ostype, sourcetab sources\n",
    "from\n",
    " (select distinct trench2.customer_id customerid,'NULL' digitalLoanAccountId,\n",
    " refno credo_ref_no,\n",
    " createdOn credo_inquiry_date, \n",
    " datetime(timestamp(createdOn), 'Asia/Manila') as credo_inquiry_date_manila ,\n",
    " date(snapshot_date) as snapshot_date, \n",
    " sourcetab, onboarding_date \n",
    " ,OFDATECLOSED\n",
    " , coalesce(credolab_ostype.ostype,'android') ostype\n",
    " from\n",
    "(select userId,refno,deviceId,createdOn,B.cust_id from  `dl_loans_db_raw.tdbk_credolab_track` A  join  `dl_customers_db_raw.tdbk_customer_mtb` B on A.userId=B.user_id )  tsa\n",
    "join prj-prod-dataplatform.core_raw.customer_accounts ca on ca.OFCUSTOMERID = tsa.cust_id and ca.CRINTERCODE = 'POC20000'\n",
    "and (ca.OFDATECLOSED = '1970-01-01' or ca.OFDATECLOSED is null)\n",
    "join  ((select * from worktable_data_analysis.All_10thAugust_snapshot)) trench2\n",
    "on tsa.cust_id=trench2.customer_id\n",
    "left join ((select distinct deviceId,'android' as ostype from credolab_raw.android_credolab_datasets_struct_columns\n",
    "union all\n",
    "select distinct deviceId,'ios' as ostype from  credolab_raw.ios_credolab_datasets_struct_columns)) credolab_ostype\n",
    " on credolab_ostype.deviceId=refno\n",
    " where  date(createdOn)<date(snapshot_date)\n",
    " qualify row_number()over (partition by cust_id order by date(createdOn) desc )=1)A\n",
    ")\n",
    "select * from a1 where credo_ref_no is not null\n",
    " \"\"\"\n",
    "\n",
    "job = client.query(sq)\n",
    "job.result()  # Wait for the job to complete.\n",
    "time.sleep(5) # Delays for 30 seconds\n",
    "print(f'Table  worktable_data_analysis.Latest_credo_before_10augustsnapshot  created successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910aa952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sq = \"\"\"with a1 as \n",
    "# (select trench2.customer_id customerid, trench2.ln_90DOB_date, trench2.onboarding_date,   \n",
    "# ca.OFCUSTOMERID, ca.CRINTERCODE, ca.OFDATECLOSED, case when ca.CRINTERCODE = 'POC20000' then 'TSA' else 'Other' end accounts\n",
    "# from worktable_data_analysis.All_90DOB_snapshot trench2\n",
    "# left join prj-prod-dataplatform.core_raw.customer_accounts ca on ca.OFCUSTOMERID = trench2.customer_id\n",
    "# )\n",
    "# select * from a1\n",
    "# ;\"\"\"\n",
    "\n",
    "# df = client.query(sq).to_dataframe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d7811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['accounts'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc455bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
